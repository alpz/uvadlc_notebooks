{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c382dc7a-e8e6-4999-8fdc-4e642bcf6795",
   "metadata": {},
   "source": [
    "# Graph Sampling for Neural Relational Inference\n",
    "\n",
    "**Notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/sampling/graphs.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/sampling/graphs.ipynb)  \n",
    "**Author:**\n",
    "Adeel Pervez\n",
    "\n",
    "\n",
    "In this tutorials we show an example of a variational autoencoder model with graph structured latent spaces.\n",
    "This model is called Neural Relational Inference and is described in this [paper](https://arxiv.org/abs/1802.04687).\n",
    "This tutorial uses code from the associated code base available [here](https://github.com/ethanfetaya/NRI)\n",
    "\n",
    "The problem dealt by this model is one of predicting particle trajectories.\n",
    "Suppose that we have $N$ interacting particles (say, charges) with some interaction structure (say, attractive/repulsive forces) that are moving about in space.\n",
    "For each particle we are observe its trajectory (say, position and velocity) as it moves about over some period of time T.\n",
    "Each new state in the tracjectory of a particle will depend on the current state (position and velocity) and on the interaction with other particles.\n",
    "Our data consists of a set of $N$ particle trajectories but the actual interactions are unknown.\n",
    "The task of our model is to learn the dynamics of the particles to predict future trajectories given example trajectories only.\n",
    "\n",
    "Notice that the task of predicting particle dynamics would become easier if we knew the form of the interactions between particles. which we think of as a graph of interactions.\n",
    "Each particle in the system would occupy a node in the graph and the strenght of the interaction could be represented by the weight of the graph.\n",
    "The interaction graph could be fed to a neural network alongwith the currently known trajectory which could then predict the next steps of the particles.\n",
    "\n",
    "However, since in this problem we are not given the interaction graph, the approach taken by Neural Relational Inference is to use the encoder of a variational autoencoder to sample a graph using the given trajectory as input.\n",
    "For this the method uses a graph neural network as encoder.\n",
    "\n",
    "## Graph Network Encoder\n",
    "Since graph neural networks already require a graph over which to pass messages, the encoder starts with a fully connected graph with each node representing a particle.\n",
    "The entire trajectory information for a particle is used as the input feature for each node.\n",
    "Node features are transformed and passed over edges and concatenated to form edge features.\n",
    "This process can be repeated to get a deeper network with edge features as the output.\n",
    "\n",
    "The edge features are then transformed into edge weights (as (unnormalized) log probabilities).\n",
    "Finally from these edge weights we can sample an interaction graph by sample edges according to their weights. \n",
    "This interaction graph then serves as a latent variable $z$ to be used in the decoder where $z_{ij}$ indicates whether edge $(i,j)$ is present in the graph.\n",
    "Since we need to backpropagate into the encoder we relax the edge sampling operation using the Gumbel-Softmax relaxation.\n",
    "\n",
    "## Graph Network Decoder\n",
    "In the decoder we use another graph network which uses the graph sampled by the encoder and the input trajectory to predict the next step in the trajectory for some predefined number of steps.\n",
    "The models uses a gaussian likelihood loss for the trajectory which is fed into the VAE loss for optimization.\n",
    "\n",
    "The model structure can be seen in the following figure from the paper linked above.\n",
    "\n",
    "![nri](img/nri.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa553a-aac7-446e-abed-fae7fed066e0",
   "metadata": {},
   "source": [
    "#Neural Relational Inference\n",
    "\n",
    "Now we build a model to learn the dynamics of a 5 particle system connected by springs.\n",
    "The trajectory data for this system has been generated synthetically using the dynamical equations of motion and a ground truth interaction graph.\n",
    "The code to load the data is given in the `load_data` function in `utils.py`.\n",
    "\n",
    "As the first step we begin with downloading loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c639a2c1-703f-435c-8291-68fdb6c4de03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-22 15:03:29--  https://surfdrive.surf.nl/files/index.php/s/6YWMO1eiVXI4EkB/download\n",
      "Resolving surfdrive.surf.nl (surfdrive.surf.nl)... 145.100.27.67, 2001:610:108:203b:0:a11:da7a:5afe\n",
      "Connecting to surfdrive.surf.nl (surfdrive.surf.nl)|145.100.27.67|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 586393709 (559M) [application/zip]\n",
      "Saving to: ‘nri_springs.zip’\n",
      "\n",
      "100%[======================================>] 586,393,709  113MB/s   in 5.1s   \n",
      "\n",
      "2022-04-22 15:03:35 (109 MB/s) - ‘nri_springs.zip’ saved [586393709/586393709]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O 'nri_springs.zip' https://surfdrive.surf.nl/files/index.php/s/6YWMO1eiVXI4EkB/download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22524751-0149-465e-8e61-9eaa4c3051e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  nri_springs.zip\n"
     ]
    }
   ],
   "source": [
    "!unzip -u nri_springs.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f4e685d-7daf-4982-89dd-57d3a21d2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "#import math\n",
    "\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import networkx\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import networkx\n",
    "except ModuleNotFoundError: \n",
    "    !pip install --quiet networkx\n",
    "    import networkx\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e387eed-b37c-42bb-8b48-253aaa47dae9",
   "metadata": {},
   "source": [
    "### Loading and Exmaining Data\n",
    "\n",
    "Next we load the data, convert the arrays into torch tensors and return the loader objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c70be800-0969-4b1c-a948-ded7afbe1517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=1, suffix=''):\n",
    "    loc_train = np.load('data/loc_train' + suffix + '.npy')\n",
    "    vel_train = np.load('data/vel_train' + suffix + '.npy')\n",
    "    edges_train = np.load('data/edges_train' + suffix + '.npy')\n",
    "\n",
    "    loc_valid = np.load('data/loc_valid' + suffix + '.npy')\n",
    "    vel_valid = np.load('data/vel_valid' + suffix + '.npy')\n",
    "    edges_valid = np.load('data/edges_valid' + suffix + '.npy')\n",
    "\n",
    "    loc_test = np.load('data/loc_test' + suffix + '.npy')\n",
    "    vel_test = np.load('data/vel_test' + suffix + '.npy')\n",
    "    edges_test = np.load('data/edges_test' + suffix + '.npy')\n",
    "\n",
    "    # [num_samples, num_timesteps, num_dims, num_atoms]\n",
    "    num_atoms = loc_train.shape[3]\n",
    "\n",
    "    loc_max = loc_train.max()\n",
    "    loc_min = loc_train.min()\n",
    "    vel_max = vel_train.max()\n",
    "    vel_min = vel_train.min()\n",
    "\n",
    "    # Normalize to [-1, 1]\n",
    "    loc_train = (loc_train - loc_min) * 2 / (loc_max - loc_min) - 1\n",
    "    vel_train = (vel_train - vel_min) * 2 / (vel_max - vel_min) - 1\n",
    "\n",
    "    loc_valid = (loc_valid - loc_min) * 2 / (loc_max - loc_min) - 1\n",
    "    vel_valid = (vel_valid - vel_min) * 2 / (vel_max - vel_min) - 1\n",
    "\n",
    "    loc_test = (loc_test - loc_min) * 2 / (loc_max - loc_min) - 1\n",
    "    vel_test = (vel_test - vel_min) * 2 / (vel_max - vel_min) - 1\n",
    "\n",
    "    # Reshape to: [num_sims, num_atoms, num_timesteps, num_dims]\n",
    "    loc_train = np.transpose(loc_train, [0, 3, 1, 2])\n",
    "    vel_train = np.transpose(vel_train, [0, 3, 1, 2])\n",
    "    feat_train = np.concatenate([loc_train, vel_train], axis=3)\n",
    "    edges_train = np.reshape(edges_train, [-1, num_atoms ** 2])\n",
    "    edges_train = np.array((edges_train + 1) / 2, dtype=np.int64)\n",
    "\n",
    "    loc_valid = np.transpose(loc_valid, [0, 3, 1, 2])\n",
    "    vel_valid = np.transpose(vel_valid, [0, 3, 1, 2])\n",
    "    feat_valid = np.concatenate([loc_valid, vel_valid], axis=3)\n",
    "    edges_valid = np.reshape(edges_valid, [-1, num_atoms ** 2])\n",
    "    edges_valid = np.array((edges_valid + 1) / 2, dtype=np.int64)\n",
    "\n",
    "    loc_test = np.transpose(loc_test, [0, 3, 1, 2])\n",
    "    vel_test = np.transpose(vel_test, [0, 3, 1, 2])\n",
    "    feat_test = np.concatenate([loc_test, vel_test], axis=3)\n",
    "    edges_test = np.reshape(edges_test, [-1, num_atoms ** 2])\n",
    "    edges_test = np.array((edges_test + 1) / 2, dtype=np.int64)\n",
    "\n",
    "    feat_train = torch.FloatTensor(feat_train)\n",
    "    edges_train = torch.LongTensor(edges_train)\n",
    "    feat_valid = torch.FloatTensor(feat_valid)\n",
    "    edges_valid = torch.LongTensor(edges_valid)\n",
    "    feat_test = torch.FloatTensor(feat_test)\n",
    "    edges_test = torch.LongTensor(edges_test)\n",
    "\n",
    "    # Exclude self edges\n",
    "    off_diag_idx = np.ravel_multi_index(\n",
    "        np.where(np.ones((num_atoms, num_atoms)) - np.eye(num_atoms)),\n",
    "        [num_atoms, num_atoms])\n",
    "    edges_train = edges_train[:, off_diag_idx]\n",
    "    edges_valid = edges_valid[:, off_diag_idx]\n",
    "    edges_test = edges_test[:, off_diag_idx]\n",
    "\n",
    "    train_data = TensorDataset(feat_train, edges_train)\n",
    "    valid_data = TensorDataset(feat_valid, edges_valid)\n",
    "    test_data = TensorDataset(feat_test, edges_test)\n",
    "\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    valid_data_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    return train_data_loader, valid_data_loader, test_data_loader, loc_max, loc_min, vel_max, vel_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7d5fe-76c5-4b34-ad67-df316ba57c55",
   "metadata": {},
   "source": [
    "We specify the batch size and the data file suffix to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a73a9f5-5e93-4195-a929-eafdf8cdbcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader, _, _, _, _ = load_data(128, \"_springs5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc632bb1-ccc5-49d1-964f-3167f42f71f7",
   "metadata": {},
   "source": [
    "Let's now examine this data. We get an iterator from the data loader and retrieve the first minibatch.\n",
    "The dataset is in the form of tuples of trajectory information and the ground truth interaction graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32dfe74b-f252-47e8-88b7-36b7bb7479d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 5, 49, 4])\n",
      "torch.Size([128, 20])\n"
     ]
    }
   ],
   "source": [
    "(x_sample,rel_sample) = next(iter(train_loader))\n",
    "print(x_sample.shape)\n",
    "print(rel_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1dc31-84db-45ef-b6a5-d62df9ea83d7",
   "metadata": {},
   "source": [
    "Let's look at the interaction graph first.\n",
    "This dataset consists of trajectories of systems of 5 particles. \n",
    "The interaction graph then specifies for each particle whether or not it interacts with every other particle.\n",
    "For 5 particles this gives us 20 interaction pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c426b3-695f-4614-8207-8203ed738a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "print(rel_sample[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78430dc-49b7-47d9-abdf-d236b5f3f66b",
   "metadata": {},
   "source": [
    "This interaction is in the form of a list of interactions pairs. We can convert one such list to an interaction graph adjancency matrix as follows. \n",
    "Here we specify that a particle does not interact with itself by setting the diagonal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e0024f5-88f5-43ff-ac3c-f15d596209ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def list_to_adj(rel):\n",
    "    b = torch.zeros((5*5))\n",
    "    for i in range(4):\n",
    "        b[i*5+i+1:(i+1)*5+(i+1)] = rel[i*5:(i+1)*5]\n",
    "    return b.reshape((5,5))\n",
    "b=list_to_adj(rel_sample[idx])\n",
    "print(b.reshape((5,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ed2c5-afa3-4269-897e-06fca2690030",
   "metadata": {},
   "source": [
    "We can draw the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be504da-6c73-4c38-8c28-dadf27637b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_graph(b):\n",
    "    g = b.reshape((5,5)).cpu().numpy()\n",
    "    graph = networkx.from_numpy_array(g)\n",
    "    networkx.draw(graph, with_labels=True)\n",
    "    plt.show()\n",
    "show_graph(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deaeca4-4231-4692-a591-1f68c10ee5f3",
   "metadata": {},
   "source": [
    "Let's now examine the trajectory data for the first data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec7c65ea-79fe-4ec8-8d1d-7d88b619404a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 49, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sample[idx].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c006d5-a5cd-443f-afba-186b71887c6e",
   "metadata": {},
   "source": [
    "The shape of the data above specifies that each entry consists of 5 particles with trajectories given for 49 time steps.\n",
    "Furthermore, each state in the trajectory is specified by a 4 dimensional vector.\n",
    "In this case the state is a 2d position and 2d velocity pair specifying the position and velocity of each particle at each time step.\n",
    "We examine the position and velocity of the first particle in the first trajectory for a couple of time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fba25cfc-42a6-4b3d-b0b1-43710d9c3b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1272, -0.0987, -0.3305,  0.1197],\n",
       "        [-0.1380, -0.0945, -0.3275,  0.1196]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sample[idx,0,0:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31f26f-5c05-46ec-8826-a89797925e6d",
   "metadata": {},
   "source": [
    "Next we define some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "718c3f7e-ad1f-44d2-bad7-68c20e23aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims=4\n",
    "num_atoms=5\n",
    "timesteps=49\n",
    "lr=0.0005\n",
    "temp=0.5\n",
    "output_var=5e-5\n",
    "\n",
    "_EPS = 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6656d05c-c319-4eac-b6c8-423350867781",
   "metadata": {},
   "source": [
    "Recall that we mentioned above that the encoder of the model works on the fully connected graph in order to predict the interaction graph.\n",
    "To pass messages over the fully connected graph, it is useful to define some relational masks specifying which vertices receive messages from which other ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7433889d-7c24-48c4-81a4-2edf202e354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "off_diag = np.ones([num_atoms, num_atoms]) - np.eye(num_atoms)\n",
    "\n",
    "rel_rec = np.array(encode_onehot(np.where(off_diag)[0]), dtype=np.float32)\n",
    "rel_send = np.array(encode_onehot(np.where(off_diag)[1]), dtype=np.float32)\n",
    "rel_rec = torch.FloatTensor(rel_rec)\n",
    "rel_send = torch.FloatTensor(rel_send)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cde3af-eb71-49a8-b244-9693f875416d",
   "metadata": {},
   "source": [
    "We can convert edge features into node features and node features into edge features by passing messages over the fully connected graph using these masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "603dcfd2-c5f8-4b9b-a154-b85f56c2826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         1., 1.]]) torch.Size([20, 5])\n"
     ]
    }
   ],
   "source": [
    "print(rel_rec.t(), rel_rec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03fa73-14bc-4625-88c1-c7f7d1944172",
   "metadata": {},
   "source": [
    "For example to convert edge features for the 20 interactions into node features we can multiply the above matrix with the edge feature vector as \n",
    "```\n",
    "torch.matmul(rel_rec.t(), x)\n",
    "```\n",
    "which collects the messages from all neighboring nodes for each vertex and adds the messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f328c-bf0c-40be-9a8d-9fc1fdbe6638",
   "metadata": {},
   "source": [
    "Next we define a simple MLP class to be used for the nonlinear feature transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cf9717f-99fd-4284-a2d9-4801cf6e878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Two-layer fully-connected ELU net with batch norm.\"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def batch_norm(self, inputs):\n",
    "        x = inputs.view(inputs.size(0) * inputs.size(1), -1)\n",
    "        x = self.bn(x)\n",
    "        return x.view(inputs.size(0), inputs.size(1), -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Input shape: [num_sims, num_things, num_features]\n",
    "        x = F.elu(self.fc1(inputs))\n",
    "        x = F.dropout(x, self.dropout_prob, training=self.training)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return self.batch_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44406999-5158-4484-a56f-6dfd423dd6fd",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Next we specify the VAE encoder as a graph neural network.\n",
    "For each particle we construct a single feature vector by using the entire trajectory information.\n",
    "We include three graph neural network layers that convert the node features into edge features into node features and finally into edge features. \n",
    "The final edge features are then used to parameterize the edge probabilities for the graph sampling operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30a127c4-0690-4358-b097-ac7f5c0c2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_out=2, do_prob=0.,):\n",
    "        super(MLPEncoder, self).__init__()\n",
    "\n",
    "        self.mlp1 = MLP(n_in, n_hid, n_hid, do_prob)\n",
    "        self.mlp2 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "        self.mlp3 = MLP(n_hid, n_hid, n_hid, do_prob)\n",
    "        self.mlp4 = MLP(n_hid * 3, n_hid, n_hid, do_prob)\n",
    "        \n",
    "        self.fc_out = nn.Linear(n_hid, n_out)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "    def edge2node(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        incoming = torch.matmul(rel_rec.t(), x)\n",
    "        return incoming / incoming.size(1)\n",
    "\n",
    "    def node2edge(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        receivers = torch.matmul(rel_rec, x)\n",
    "        senders = torch.matmul(rel_send, x)\n",
    "        edges = torch.cat([senders, receivers], dim=2)\n",
    "        return edges\n",
    "\n",
    "    def forward(self, inputs, rel_rec, rel_send):\n",
    "        # Input shape: [num_sims, num_atoms, num_timesteps, num_dims]\n",
    "        x = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "        # New shape: [num_sims, num_atoms, num_timesteps*num_dims]\n",
    "\n",
    "        x = self.mlp1(x)  # 2-layer ELU net per node\n",
    "\n",
    "        x = self.node2edge(x, rel_rec, rel_send)\n",
    "        x = self.mlp2(x)\n",
    "        x_skip = x\n",
    "\n",
    "        x = self.edge2node(x, rel_rec, rel_send)\n",
    "        x = self.mlp3(x)\n",
    "        x = self.node2edge(x, rel_rec, rel_send)\n",
    "        x = torch.cat((x, x_skip), dim=2)  # Skip connection\n",
    "        x = self.mlp4(x)\n",
    "\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b92015-2ce8-45c6-bf59-c7a99120171d",
   "metadata": {},
   "source": [
    "### Decoder \n",
    "\n",
    "In the decoder we take the initial state and attempt to predict the remaining trajectory.\n",
    "To predict a single time step the function `single_step_forward` takes as input the current state and the proposed interaction graph.\n",
    "From the states of the nodes as node features we produce edge features by passing messages over all neighbors and then zero out the messages corresponding to edges not present in the proposed graph.\n",
    "The edge messages are then sent to the corresponding nodes after which we apply an MLP to compute the next prediction as a difference with the current state.\n",
    "The process is then repeated for all timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "448792ed-f196-464d-81db-e3edab31a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoder(nn.Module):\n",
    "    \"\"\"MLP decoder module.\"\"\"\n",
    "\n",
    "    def __init__(self, n_in_node, edge_types, msg_hid, msg_out, n_hid,\n",
    "                 do_prob=0.):\n",
    "        super(MLPDecoder, self).__init__()\n",
    "\n",
    "        self.msg_fc1 = (nn.Linear(2 * n_in_node, msg_hid))\n",
    "        self.msg_fc2 = (nn.Linear(msg_hid, msg_out))\n",
    "        self.msg_out_shape = msg_out\n",
    "\n",
    "        self.out_fc1 = nn.Linear(n_in_node + msg_out, n_hid)\n",
    "        self.out_fc2 = nn.Linear(n_hid, n_hid)\n",
    "        self.out_fc3 = nn.Linear(n_hid, n_in_node)\n",
    "\n",
    "        print('Using learned interaction net decoder.')\n",
    "\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "    def single_step_forward(self, single_timestep_inputs, rel_rec, rel_send,\n",
    "                            single_timestep_rel_type):\n",
    "\n",
    "        # single_timestep_inputs has shape\n",
    "        # [batch_size, num_timesteps, num_atoms, num_dims]\n",
    "\n",
    "        # single_timestep_rel_type has shape:\n",
    "        # [batch_size, num_timesteps, num_atoms*(num_atoms-1), num_edge_types]\n",
    "\n",
    "        # Node2edge\n",
    "        receivers = torch.matmul(rel_rec, single_timestep_inputs)\n",
    "        senders = torch.matmul(rel_send, single_timestep_inputs)\n",
    "        pre_msg = torch.cat([senders, receivers], dim=-1)\n",
    "\n",
    "        msg = F.relu(self.msg_fc1(pre_msg))\n",
    "        msg = F.dropout(msg, p=self.dropout_prob)\n",
    "        msg = F.relu(self.msg_fc2(msg))\n",
    "        msg = msg * single_timestep_rel_type[:, :, :, 1:2]\n",
    "\n",
    "        # Aggregate all msgs to receiver\n",
    "        agg_msgs = msg.transpose(-2, -1).matmul(rel_rec).transpose(-2, -1)\n",
    "        agg_msgs = agg_msgs.contiguous()\n",
    "\n",
    "        # Skip connection\n",
    "        aug_inputs = torch.cat([single_timestep_inputs, agg_msgs], dim=-1)\n",
    "\n",
    "        # Output MLP\n",
    "        pred = F.dropout(F.relu(self.out_fc1(aug_inputs)), p=self.dropout_prob)\n",
    "        pred = F.dropout(F.relu(self.out_fc2(pred)), p=self.dropout_prob)\n",
    "        pred = self.out_fc3(pred)\n",
    "\n",
    "        # Predict position/velocity difference\n",
    "        return single_timestep_inputs + pred\n",
    "\n",
    "    def forward(self, inputs, rel_type, rel_rec, rel_send, pred_steps=1):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "\n",
    "        inputs = inputs.transpose(1, 2).contiguous()\n",
    "\n",
    "        sizes = [rel_type.size(0), inputs.size(1), rel_type.size(1),\n",
    "                 rel_type.size(2)]\n",
    "        rel_type = rel_type.unsqueeze(1).expand(sizes)\n",
    "\n",
    "        time_steps = inputs.size(1)\n",
    "        assert (pred_steps <= time_steps)\n",
    "        preds = []\n",
    "\n",
    "        # initial step\n",
    "        last_pred = inputs[:, 0:1, :, :]\n",
    "        # NOTE: Assumes rel_type is constant (i.e. same across all time steps).\n",
    "        curr_rel_type = rel_type[:, 0:1, :, :]\n",
    "\n",
    "        # Run n prediction steps\n",
    "        for step in range(0, pred_steps):\n",
    "            last_pred = self.single_step_forward(last_pred, rel_rec, rel_send,\n",
    "                                                 curr_rel_type)\n",
    "            preds.append(last_pred)\n",
    "\n",
    "        sizes = [preds[0].size(0), preds[0].size(1) * pred_steps,\n",
    "                 preds[0].size(2), preds[0].size(3)]\n",
    "\n",
    "        #output = Variable(torch.zeros(sizes))\n",
    "        output = torch.zeros(sizes)\n",
    "        if inputs.is_cuda:\n",
    "            output = output.cuda()\n",
    "\n",
    "        # Re-assemble correct timeline\n",
    "        for i in range(len(preds)):\n",
    "            output[:, i:i+1, :, :] = preds[i]\n",
    "\n",
    "        pred_all = output[:, :(inputs.size(1) - 1), :, :]\n",
    "\n",
    "        return pred_all.transpose(1, 2).contiguous()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34642bf5-60b1-4dc9-bf3f-276ffb0e93cd",
   "metadata": {},
   "source": [
    "Now we make a few helper arrays to specify the sending and receiving nodes in a message passing step. These simply correspond to the neighbors in a fully connected graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fddcd409-80ac-452a-89e9-f91e1caba245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate off-diagonal interaction graph\n",
    "off_diag = np.ones([num_atoms, num_atoms]) - np.eye(num_atoms)\n",
    "\n",
    "rel_rec = np.array(encode_onehot(np.where(off_diag)[0]), dtype=np.float32)\n",
    "rel_send = np.array(encode_onehot(np.where(off_diag)[1]), dtype=np.float32)\n",
    "rel_rec = torch.FloatTensor(rel_rec)\n",
    "rel_send = torch.FloatTensor(rel_send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4634d87c-9724-440b-a12e-4b672883189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using learned interaction net decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apervez/anaconda3/envs/pytorch1.9_2/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#edge weights have dim 2\n",
    "encoder = MLPEncoder(timesteps * dims, 256, 2)\n",
    "decoder = MLPDecoder(n_in_node=dims,\n",
    "                         edge_types=2,\n",
    "                         msg_hid=256,\n",
    "                         msg_out=256,\n",
    "                         n_hid=256, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aedba580-7db9-4123-89ee-b6794114a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.cuda()\n",
    "decoder.cuda()\n",
    "rel_rec = rel_rec.cuda()\n",
    "rel_send = rel_send.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d039d23a-9a0e-4f8f-a2f3-90ced16b8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),\n",
    "                       lr=lr)\n",
    "train_loader, valid_loader, test_loader, loc_max, loc_min, vel_max, vel_min = load_data(\n",
    "    128, \"_springs5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ae7cf-d6ba-49cd-9fd7-ae410d7e7eb0",
   "metadata": {},
   "source": [
    "### Discrete Sampling\n",
    "\n",
    "We will sample graphs by selecting a subset of the fully connected graph using the weights output by the encoder by using Gumbel softmax.\n",
    "We define the Gumbel softmax routines below as from the earlier tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fbc6909-b5ac-4e45-a369-6eab38d93200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(input, axis=1):\n",
    "    trans_input = input.transpose(axis, 0).contiguous()\n",
    "    soft_max_1d = F.softmax(trans_input)\n",
    "    return soft_max_1d.transpose(axis, 0)\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-10):\n",
    "    U = torch.rand(shape).float()\n",
    "    return - torch.log(eps - torch.log(U + eps))\n",
    "\n",
    "def gumbel_softmax_sample(logits, tau=1, eps=1e-10):\n",
    "    gumbel_noise = sample_gumbel(logits.size(), eps=eps)\n",
    "    if logits.is_cuda:\n",
    "        gumbel_noise = gumbel_noise.cuda()\n",
    "    y = logits + Variable(gumbel_noise)\n",
    "    return my_softmax(y / tau, axis=-1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10):\n",
    "    y_soft = gumbel_softmax_sample(logits, tau=tau, eps=eps)\n",
    "    if hard:\n",
    "        shape = logits.size()\n",
    "        _, k = y_soft.data.max(-1)\n",
    "        y_hard = torch.zeros(*shape)\n",
    "        if y_soft.is_cuda:\n",
    "            y_hard = y_hard.cuda()\n",
    "        y_hard = y_hard.zero_().scatter_(-1, k.view(shape[:-1] + (1,)), 1.0)\n",
    "        y = Variable(y_hard - y_soft.data) + y_soft\n",
    "    else:\n",
    "        y = y_soft\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a3386-364b-42b4-876b-e377f2e17da6",
   "metadata": {},
   "source": [
    "Next we define some helper functions for computing accuracies, prediction loss and KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15817d21-88b9-47f2-9457-39a93809b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kl_categorical_uniform(preds, num_atoms, num_edge_types, add_const=False,\n",
    "                           eps=1e-16):\n",
    "    kl_div = preds * torch.log(preds + eps)\n",
    "    if add_const:\n",
    "        const = np.log(num_edge_types)\n",
    "        kl_div += const\n",
    "    return kl_div.sum() / (num_atoms * preds.size(0))\n",
    "\n",
    "\n",
    "def nll_gaussian(preds, target, variance, add_const=False):\n",
    "    neg_log_p = ((preds - target) ** 2 / (2 * variance))\n",
    "    if add_const:\n",
    "        const = 0.5 * np.log(2 * np.pi * variance)\n",
    "        neg_log_p += const\n",
    "    return neg_log_p.sum() / (target.size(0) * target.size(1))\n",
    "\n",
    "def edge_accuracy(preds, target):\n",
    "    _, preds = preds.max(-1)\n",
    "    correct = preds.float().data.eq(\n",
    "        target.float().data.view_as(preds)).cpu().sum()\n",
    "    return np.float(correct) / (target.size(0) * target.size(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a73cc88-963d-418c-8311-dc084a59946c",
   "metadata": {},
   "source": [
    "We can now train the model.\n",
    "This runs the encoder to get the latent graph parameters and samples the edges using the `gumbel_softmax` function to get a latent graph which is then passed to the decoder.\n",
    "We use the uniform categorical prior to compute the KL divergence for the VAE loss and a Gaussian likelihood loss with a fixed various for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6e242e8-6bf6-4b24-ab19-25e42f5683b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, best_val_loss):\n",
    "    t = time.time()\n",
    "    nll_train = []\n",
    "    acc_train = []\n",
    "    kl_train = []\n",
    "    mse_train = []\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    #scheduler.step()\n",
    "    for batch_idx, (data, relations) in enumerate(train_loader):\n",
    "\n",
    "        #if args.cuda:\n",
    "        data, relations = data.cuda(), relations.cuda()\n",
    "        #data, relations = Variable(data), Variable(relations)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = encoder(data, rel_rec, rel_send)\n",
    "        edges = gumbel_softmax(logits, tau=temp, hard=False)\n",
    "        prob = my_softmax(logits, -1)\n",
    "\n",
    "        output = decoder(data, edges, rel_rec, rel_send, timesteps)\n",
    "\n",
    "        target = data[:, :, 1:, :]\n",
    "\n",
    "        loss_nll = nll_gaussian(output, target, output_var)\n",
    "\n",
    "        loss_kl = kl_categorical_uniform(prob, num_atoms, 2)\n",
    "\n",
    "        loss = loss_nll + loss_kl\n",
    "\n",
    "        acc = edge_accuracy(logits, relations)\n",
    "        acc_train.append(acc)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mse_train.append(F.mse_loss(output, target).item())\n",
    "        nll_train.append(loss_nll.item())\n",
    "        kl_train.append(loss_kl.item())\n",
    "\n",
    "    nll_val = []\n",
    "    acc_val = []\n",
    "    kl_val = []\n",
    "    mse_val = []\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for batch_idx, (data, relations) in enumerate(valid_loader):\n",
    "        #if args.cuda:\n",
    "        data, relations = data.cuda(), relations.cuda()\n",
    "\n",
    "        logits = encoder(data, rel_rec, rel_send)\n",
    "        edges = gumbel_softmax(logits, tau=temp, hard=True)\n",
    "        prob = my_softmax(logits, -1)\n",
    "\n",
    "        # validation output uses teacher forcing\n",
    "        output = decoder(data, edges, rel_rec, rel_send, timesteps)\n",
    "\n",
    "        target = data[:, :, 1:, :]\n",
    "        loss_nll = nll_gaussian(output, target, output_var)\n",
    "\n",
    "        loss_kl = kl_categorical_uniform(prob, num_atoms, 2)\n",
    "\n",
    "        acc = edge_accuracy(logits, relations)\n",
    "        acc_val.append(acc)\n",
    "\n",
    "        mse_val.append(F.mse_loss(output, target).item())\n",
    "        nll_val.append(loss_nll.item())\n",
    "        kl_val.append(loss_kl.item())\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch),\n",
    "          'nll_train: {:.10f}'.format(np.mean(nll_train)),\n",
    "          'kl_train: {:.10f}'.format(np.mean(kl_train)),\n",
    "          'mse_train: {:.10f}'.format(np.mean(mse_train)),\n",
    "          'acc_train: {:.10f}'.format(np.mean(acc_train)),\n",
    "          'nll_val: {:.10f}'.format(np.mean(nll_val)),\n",
    "          'kl_val: {:.10f}'.format(np.mean(kl_val)),\n",
    "          'mse_val: {:.10f}'.format(np.mean(mse_val)),\n",
    "          'acc_val: {:.10f}'.format(np.mean(acc_val)),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    return np.mean(nll_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689471d-3244-45d3-8935-d73b83e76c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apervez/anaconda3/envs/pytorch1.9_2/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 nll_train: 82207.3378531610 kl_train: -0.4316625203 mse_train: 0.0428163237 acc_train: 0.5102473625 nll_val: 13553.7608163568 kl_val: -0.5440562605 mse_val: 0.0070592507 acc_val: 0.5207624604 time: 39.0780s\n",
      "Epoch: 0001 nll_train: 10482.0866405750 kl_train: -0.8068965521 mse_train: 0.0054594203 acc_train: 0.5672370524 nll_val: 9608.4652764043 kl_val: -0.9848571722 mse_val: 0.0050044091 acc_val: 0.6150959256 time: 38.8682s\n",
      "Epoch: 0002 nll_train: 7622.5237534467 kl_train: -0.9681852530 mse_train: 0.0039700646 acc_train: 0.6575051950 nll_val: 7999.2846617880 kl_val: -0.9407296845 mse_val: 0.0041662942 acc_val: 0.6889833861 time: 38.9219s\n",
      "Epoch: 0003 nll_train: 6435.7686271180 kl_train: -0.8848858903 mse_train: 0.0033519629 acc_train: 0.7282261029 nll_val: 6645.1351859177 kl_val: -0.8726646719 mse_val: 0.0034610081 acc_val: 0.7700009889 time: 38.9155s\n",
      "Epoch: 0004 nll_train: 5131.3655265945 kl_train: -0.7572590768 mse_train: 0.0026725863 acc_train: 0.8130400815 nll_val: 4324.9007367484 kl_val: -0.6718307175 mse_val: 0.0022525526 acc_val: 0.8733831092 time: 38.8255s\n",
      "Epoch: 0005 nll_train: 3077.5661589874 kl_train: -0.5434278285 mse_train: 0.0016028991 acc_train: 0.8900433584 nll_val: 3308.9670224733 kl_val: -0.4739881709 mse_val: 0.0017234204 acc_val: 0.9016811709 time: 38.8303s\n",
      "Epoch: 0006 nll_train: 2414.1095929158 kl_train: -0.4157265660 mse_train: 0.0012573488 acc_train: 0.9119279492 nll_val: 2641.4135803995 kl_val: -0.3809240079 mse_val: 0.0013757363 acc_val: 0.9172320016 time: 38.7057s\n",
      "Epoch: 0007 nll_train: 2069.5431726168 kl_train: -0.3398666841 mse_train: 0.0010778871 acc_train: 0.9245336477 nll_val: 2234.7316940887 kl_val: -0.3186675578 mse_val: 0.0011639228 acc_val: 0.9260087025 time: 38.7296s\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "best_val_loss = np.inf\n",
    "best_epoch = 0\n",
    "for epoch in range(10):\n",
    "    val_loss = train(epoch, best_val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990669f-4925-4430-83a5-bb4cdcef4d8b",
   "metadata": {},
   "source": [
    "### Visualizing Discovered Graphs\n",
    "We can now visualize the actual and predicted interaction graphs for some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead91d0b-0338-41a0-bf53-ffc1432e53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data,relations = next(iter(valid_loader))\n",
    "data=data.cuda()\n",
    "relations=relations.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe574b-baa2-44a1-848a-f7bc4a43079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = encoder(data, rel_rec, rel_send)\n",
    "_, rel = logits.max(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e69a5-49e5-466b-8d6b-80feae023ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rel[0])\n",
    "print(relations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ed6b6-9cec-4181-b78f-8903a07e7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    g_act = list_to_adj(relations[i])\n",
    "    g_pred = list_to_adj(rel[i])\n",
    "\n",
    "    print(\"Original\")\n",
    "    show_graph(g_act)\n",
    "    print(\"Predicted\")\n",
    "    show_graph(g_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc8431-e138-4388-9bb3-87bb5628037d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Neural Relational Inference for Interacting Systems](https://arxiv.org/abs/1802.04687)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f993e-728e-41c9-9786-ab9d0ca08140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
